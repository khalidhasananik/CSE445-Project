{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoost Model for Dengue Prediction\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('/home/anik/CSE445-Project/DenguePrediction/dataset/dataset.csv', header = None)\n",
    "dataset.columns = dataset.iloc[0]\n",
    "dataset = dataset[1:]\n",
    "dataset = dataset.drop(columns=['Year'])\n",
    "\n",
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "dataset['District'] = label_encoder.fit_transform(dataset['District'])\n",
    "\n",
    "# Handling missing data\n",
    "dataset = dataset.apply(pd.to_numeric, errors='coerce')\n",
    "# dataset.ffill(inplace=True)\n",
    "# dataset.fillna(dataset.mean(), inplace=True)\n",
    "dataset.fillna(dataset.mode().iloc[0], inplace=True)\n",
    "# dataset.fillna(dataset.median(), inplace=True)\n",
    "dataset['Total Cases'] = dataset.iloc[:, 37:49].sum(axis=1)\n",
    "dataset = dataset.drop(dataset.columns[37:49], axis=1)\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = dataset.iloc[:, 0:-1].values\n",
    "Y = dataset.iloc[:, -1].values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameter tuning(Grid Search)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# parameters = {\n",
    "#     'n_estimators': [50, 100, 150, 200, 250],\n",
    "#     'learning_rate': [0.01, 0.1, 0.5, 1],\n",
    "#     'loss': ['linear', 'square', 'exponential'],\n",
    "# }\n",
    "# grid_search = GridSearchCV(estimator = AdaBoostRegressor(), param_grid = parameters, scoring = 'r2', cv = 10, n_jobs = -1)\n",
    "# grid_search = grid_search.fit(X_train, Y_train)\n",
    "# best_parameters = grid_search.best_params_\n",
    "\n",
    "# Predict on the test set(grid search)\n",
    "#Y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "#Y_pred = AdaBoostRegressor().fit(X_train, Y_train).predict(X_test)\n",
    "\n",
    "# Hyperparameter tuning(Random Search)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "parameters = {\n",
    "    'n_estimators': randint(50, 250),\n",
    "    'learning_rate': [0.01, 0.1, 0.5, 1],\n",
    "    'loss': ['linear', 'square', 'exponential'],\n",
    "}\n",
    "random_search = RandomizedSearchCV(estimator = AdaBoostRegressor(), param_distributions = parameters, n_iter = 100, scoring = 'r2', cv = 10, n_jobs = -1)\n",
    "random_search = random_search.fit(X_train, Y_train)\n",
    "best_parameters = random_search.best_params_\n",
    "\n",
    "# Predict on the test set(random search)\n",
    "# Y_pred = random_search.best_estimator_.predict(X_test)\n",
    "Y_pred = AdaBoostRegressor().fit(X_train, Y_train).predict(X_test)\n",
    "\n",
    "# Hyperparameter tuning(Bayesian Optimization)\n",
    "from skopt import BayesSearchCV\n",
    "# parameters = {\n",
    "#     'n_estimators': (50, 250),\n",
    "#     'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
    "#     'loss': ['linear', 'square', 'exponential'],\n",
    "# }\n",
    "# bayes_search = BayesSearchCV(estimator = AdaBoostRegressor(), search_spaces = parameters, n_iter = 100, scoring = 'r2', cv = 10, n_jobs = -1)\n",
    "# bayes_search = bayes_search.fit(X_train, Y_train)\n",
    "# best_parameters = bayes_search.best_params_\n",
    "\n",
    "# Predict on the test set(bayesian optimization)\n",
    "# Y_pred = bayes_search.best_estimator_.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(f\"R2 Score: {r2_score(Y_test, Y_pred)}\")\n",
    "print(f\"MSE: {mean_squared_error(Y_test, Y_pred)}\")\n",
    "\n",
    "# Explainaing the model with SHAP\n",
    "import shap\n",
    "explainer = shap.KernelExplainer(random_search.best_estimator_.predict, X_train)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test, feature_names=dataset.columns[:-1])\n",
    "\n",
    "# Explainaing the model with LIME\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train, mode='regression', feature_names=dataset.columns[:-1])\n",
    "i = 0\n",
    "exp = explainer.explain_instance(X_test[i], random_search.best_estimator_.predict, num_features=5)\n",
    "exp.show_in_notebook(show_table=True)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
